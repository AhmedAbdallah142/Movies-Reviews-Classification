{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk\n!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T13:36:12.605493Z","iopub.execute_input":"2022-02-07T13:36:12.606017Z","iopub.status.idle":"2022-02-07T13:36:28.368342Z","shell.execute_reply.started":"2022-02-07T13:36:12.605975Z","shell.execute_reply":"2022-02-07T13:36:28.367370Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport nltk\nimport string\nfrom torch import nn\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom transformers import BertModel, BertTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.stem.porter import PorterStemmer","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:36:45.679738Z","iopub.execute_input":"2022-02-07T13:36:45.680028Z","iopub.status.idle":"2022-02-07T13:36:45.685312Z","shell.execute_reply.started":"2022-02-07T13:36:45.679998Z","shell.execute_reply":"2022-02-07T13:36:45.684135Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# read data from drive\n# original data   : 1pqa8tsY5kP6AmWRiXazkk1rHIewe40UH\n# small data : 1K6xnL8mOENuqgJmSmrr3RiLbuI7U1aa0\n\ndf = pd.read_csv('https://drive.google.com/uc?export=view&id=1pqa8tsY5kP6AmWRiXazkk1rHIewe40UH')\nprint(f'size : {len(df)}\\n', df.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:37:32.600995Z","iopub.execute_input":"2022-02-07T13:37:32.601621Z","iopub.status.idle":"2022-02-07T13:37:35.164245Z","shell.execute_reply.started":"2022-02-07T13:37:32.601587Z","shell.execute_reply":"2022-02-07T13:37:35.163476Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# truncate the data\nnew_size = 1000\nnew_size = int(new_size/2) \n\npos = df[df['sentiment'] == 'positive'].iloc[0:new_size, :]\nneg = df[df['sentiment'] == 'negative'].iloc[0:new_size, :]\ndf = pos.append(neg).sample(frac= 1)\nprint(f'size : {len(df)}\\n', df.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:37:25.473509Z","iopub.execute_input":"2022-02-07T13:37:25.473938Z","iopub.status.idle":"2022-02-07T13:37:25.525521Z","shell.execute_reply.started":"2022-02-07T13:37:25.473902Z","shell.execute_reply":"2022-02-07T13:37:25.524635Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# data preprocessing\nnltk.download('stopwords')\nnltk.download('punkt')\nporter =  PorterStemmer()\nstop_words = stopwords.words('english')\ndef preprocess(text):\n    text = text.lower()\n    text = \"\".join([char for char in text if char not in string.punctuation])\n    text = word_tokenize(text)\n    text = \" \".join([porter.stem(word) for word in text if word not in stop_words])\n    return text;\ndf['review'] = df['review'].apply(preprocess)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:37:39.925933Z","iopub.execute_input":"2022-02-07T13:37:39.926185Z","iopub.status.idle":"2022-02-07T13:42:14.664325Z","shell.execute_reply.started":"2022-02-07T13:37:39.926157Z","shell.execute_reply":"2022-02-07T13:42:14.663604Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# split the data\npos = df[df['sentiment'] == 'positive']\nneg = df[df['sentiment'] == 'negative']\n\nprint(len(pos), len(neg))\n\npos_train, pos_val, pos_test = np.split(pos,\n                                     [int(.7*len(pos)), int(.8*len(pos))])\n\nneg_train, neg_val, neg_test = np.split(neg, \n                                        [int(.7*len(neg)), int(.8*len(neg))])\n\nprint(len(pos_train),len(pos_val), len(pos_test))\nprint(len(neg_train),len(neg_val), len(neg_test))\nprint(\"\")\n\ndf_train = pos_train.append(neg_train).sample(frac= 1)\ndf_test = pos_test.append(neg_test).sample(frac= 1)\ndf_val = pos_val.append(neg_val).sample(frac = 1)\nprint(len(df_train), df_train.head(), end=\"\\n\\n\")\nprint(len(df_val), df_val.head(), end=\"\\n\\n\")\nprint(len(df_test), df_test.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:42:26.841913Z","iopub.execute_input":"2022-02-07T13:42:26.842186Z","iopub.status.idle":"2022-02-07T13:42:26.907558Z","shell.execute_reply.started":"2022-02-07T13:42:26.842155Z","shell.execute_reply":"2022-02-07T13:42:26.906659Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nlabels = {'negative':0,'positive':1}\n\nclass Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, df):\n\n        self.labels = [labels[label] for label in df['sentiment']]\n        self.texts = [tokenizer(text, \n                               padding='max_length', max_length = 512, truncation=True,\n                                return_tensors=\"pt\") for text in df['review']]\n\n    def classes(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_batch_labels(self, idx):\n        # Fetch a batch of labels\n        return np.array(self.labels[idx])\n\n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n\n        return batch_texts, batch_y","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:42:32.886535Z","iopub.execute_input":"2022-02-07T13:42:32.886947Z","iopub.status.idle":"2022-02-07T13:42:36.179954Z","shell.execute_reply.started":"2022-02-07T13:42:32.886913Z","shell.execute_reply":"2022-02-07T13:42:36.179161Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n\n    def __init__(self, dropout=0.5):\n\n        super(BertClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-cased')\n        self.dropout = nn.Dropout(dropout)\n        self.linear1 = nn.Linear(768, 512)\n        self.linear2 = nn.Linear(512, 256)\n        self.linear3 = nn.Linear(256, 128)\n        self.linear4 = nn.Linear(128, 64)\n        self.linear5 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_id, mask):\n\n        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear1(dropout_output)\n        linear_output = self.linear2(linear_output)\n        linear_output = self.linear3(linear_output)\n        linear_output = self.linear4(linear_output)\n        linear_output = self.linear5(linear_output)\n        final_layer = self.sigmoid(linear_output)\n        return final_layer","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:42:40.490170Z","iopub.execute_input":"2022-02-07T13:42:40.490559Z","iopub.status.idle":"2022-02-07T13:42:40.499152Z","shell.execute_reply.started":"2022-02-07T13:42:40.490525Z","shell.execute_reply":"2022-02-07T13:42:40.498465Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def train(model, train_data, val_data, learning_rate, epochs):\n\n    train, val = Dataset(train_data), Dataset(val_data)\n\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=8, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=8)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    criterion = nn.BCELoss()\n    optimizer = Adam(model.parameters(), lr= learning_rate)\n\n    if use_cuda:\n\n            model = model.cuda()\n            criterion = criterion.cuda()\n\n    for epoch_num in range(epochs):\n\n            total_acc_train = 0\n            total_loss_train = 0\n\n            for train_input, train_label in tqdm(train_dataloader):\n                acc = 0\n                train_label = train_label.to(device).to(torch.float32)\n                mask = train_input['attention_mask'].to(device)\n                input_id = train_input['input_ids'].squeeze(1).to(device)\n\n                output = model(input_id, mask)\n                \n                batch_loss = criterion(output, train_label.unsqueeze(1))\n                total_loss_train += batch_loss.item()\n                \n                output = (output > 0.5).float()\n                for i in range(len(output)) :\n                    if (output[i] == train_label[i]):\n                        acc += 1\n#                 acc = (output == train_label).sum().item()\n#                 print(output , \"output - train\", train_label , \"Acc = \" , acc)\n                total_acc_train += acc\n\n                model.zero_grad()\n                batch_loss.backward()\n                optimizer.step()\n            \n            total_acc_val = 0\n            total_loss_val = 0\n\n            with torch.no_grad():\n\n                for val_input, val_label in val_dataloader:\n                    acc = 0\n                    val_label = val_label.to(device).to(torch.float32)\n                    mask = val_input['attention_mask'].to(device)\n                    input_id = val_input['input_ids'].squeeze(1).to(device)\n\n                    output = model(input_id, mask)\n                    batch_loss = criterion(output, val_label.unsqueeze(1))\n                    total_loss_val += batch_loss.item()\n                    \n                    output = (output > 0.5).float()\n                    for i in range(len(output)) :\n                        if (output[i] == val_label[i]):\n                            acc += 1\n#                     acc = (output == val_label).sum().item()\n                    \n                    total_acc_val += acc\n            \n            print(\"train: \" , total_acc_train )\n            print(\"val: \" , total_acc_val )\n            print(\n                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n                  \nEPOCHS = 5\nmodel = BertClassifier()\nLR = 1e-6\n              \ntrain(model, df_train, df_val, LR, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:42:44.797899Z","iopub.execute_input":"2022-02-07T13:42:44.798310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, test_data):\n\n    test = Dataset(test_data)\n\n    test_dataloader = torch.utils.data.DataLoader(test, batch_size=8)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n\n        model = model.cuda()\n\n    tp, tn, fp, fn = 0,0,0,0\n    with torch.no_grad():\n        for test_input, test_label in test_dataloader:\n            test_label = test_label.to(device)\n            mask = test_input['attention_mask'].to(device)\n            input_id = test_input['input_ids'].squeeze(1).to(device)\n            output = model(input_id, mask)\n            output = torch.round(output) \n            for i in range(len(output)) :\n                if(output[i] == test_label[i]) :\n                    if (output[i] == 1) : \n                        tp += 1\n                    else:\n                        tn += 1\n                else:\n                    if (output[i] == 1) : \n                        fp += 1\n                    else:\n                        fn += 1\n    print(\"Confusion Matrix :-\")\n    print(f\"{tp}   {fp}\")\n    print(f\"{fn}   {tn}\")\n    precision, recall = (tp)/(tp+fp), (tp)/(tp+fn)\n    print(f\"Accuracy percentage is: {(tp+tn)/(tp+tn+fp+fn):.3f}\")\n    print(f\"Specificity percentage is: {(tn)/(tn+fp):.3f} %\")\n    print(f\"Precision is: {precision:.3f}\")\n    print(f\"Recall is: {recall :.3f}\")\n    print(f\"F1 score is: {2 * (precision * recall) / (precision + recall):.3f}\")\n    \nevaluate(model, df_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T10:48:54.551353Z","iopub.execute_input":"2022-02-07T10:48:54.551905Z","iopub.status.idle":"2022-02-07T10:48:54.639531Z","shell.execute_reply.started":"2022-02-07T10:48:54.551813Z","shell.execute_reply":"2022-02-07T10:48:54.638635Z"},"trusted":true},"execution_count":null,"outputs":[]}]}